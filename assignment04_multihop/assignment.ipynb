{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-n7dD4smAM"
      },
      "source": [
        "# Assignment â€” Multihop query answering\n",
        "\n",
        "In this task, we will implement the [betaE](http://snap.stanford.edu/betae/) model. In comparison to the Query2Box, it is able to handle the negation operator\n",
        "\n",
        "Firstly, we need to download data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzr6KYrZ2A7k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zlib import adler32\n",
        "from tqdm.notebook import trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhP5UTVO1-wx"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/countries_edges.tsv'\n",
        "open('countries_edges.tsv', 'wb').write(requests.get(url).content)\n",
        "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/countries_entities.tsv'\n",
        "open('countries_entities.tsv', 'wb').write(requests.get(url).content)\n",
        "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/countries_relations.tsv'\n",
        "open('countries_relations.tsv', 'wb').write(requests.get(url).content);\n",
        "\n",
        "\n",
        "edges = pd.read_csv('countries_edges.tsv', sep='\\t').values\n",
        "entity_labels = pd.read_csv('countries_entities.tsv', sep='\t', index_col=0).label.values\n",
        "relation_labels = pd.read_csv('countries_relations.tsv', sep='\t', index_col=0).label.values\n",
        "\n",
        "edges_labeled = np.stack([entity_labels[edges[:, 0]], \n",
        "                          entity_labels[edges[:, 1]], \n",
        "                          relation_labels[edges[:, 2]]], axis=1)\n",
        "\n",
        "df = pd.DataFrame(edges_labeled, columns=['h', 't', 'r'])[['h', 'r', 't']]\n",
        "df = df[df.h != df.t].reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beJG9nrOKRc2"
      },
      "source": [
        "Now, let us encode the entity names with indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlxPcFNbiTO1"
      },
      "outputs": [],
      "source": [
        "ent2id = sorted(list(set(df.h) | set(df.t)))\n",
        "ent2id = dict(zip(ent2id, range(len(ent2id))))\n",
        "\n",
        "rel2id = sorted(list(set(df.r)))\n",
        "rel2id = dict(zip(rel2id, range(len(rel2id))))\n",
        "\n",
        "df.h = df.h.map(ent2id)\n",
        "df.t = df.t.map(ent2id)\n",
        "df.r = df.r.map(rel2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfxvOVxJKRc3"
      },
      "source": [
        "## Task 1. Mine queries (2 points)\n",
        "\n",
        "To train our model, we need to mine the queries for the training and validation of our model. In this task, we will consider only queries of two types:\n",
        "\n",
        "1. intersection of two projections ($V_?: Relation_1(V_?, Anchor_1) \\land Relation_2(V_?, Anchor_2)$). For example, let us consider a query on natural language.  List the European countries that have held the World Cup. It could be converted to the logical statement: $V_?: Located(Europe, V_?) \\land Held(Word Cup, V_?)$.\n",
        "1. intersection of projection and negation of projection ($V_?: Relation_1(Anchor_1, V_?) \\land \\neg Relation_2(Anchor_2, V_?)$). For example, let us consider a query on natural language.  List the European countries that have __never__ held the World Cup. It could be converted to the logical statement: $V_?: Located(Europe, V_?) \\land \\neg Held(Word Cup, V_?)$.\n",
        "\n",
        "\n",
        "To find such queries, we will use the `mine_intersection_and_negation` method.\n",
        "\n",
        "The first type of query was described on the [seminar](https://github.com/netspractice/advanced_gnn/blob/main/lab_multihop/lab.ipynb) in the method `generete_queries_conjunction`. The general intuition is to find head and relation pairs that project to the same tails. One way to handle it is to join our triplets on itself, keyed by the tail column. It will return the dataset with two head-relation pairs and a similar tail.\n",
        "\n",
        "The second type of query has a negation, and we can find it similarly. Firstly, we will find such head-relation pairs that lead to similar tails. Secondly, we will find all possible answers for one joined head-relation pair. Finally, we will remove the intersection from the answers found before.\n",
        "\n",
        "\n",
        "The method `mine_intersection_and_negation` should work as follows:\n",
        "\n",
        "The first three steps are similar to the `generate_queries_conjunction` method from the seminar.\n",
        "\n",
        "1. Merge triplet dataframe on itself by column \"t\" (let us call it `df_merged`)\n",
        "2. Remove lines from `df_merged` where heads and tails (from left and right parts after join) are similar\n",
        "3. Group the `df_merged` on heads and relations from both parts of the dataset and aggregate tails as a list (let us call it `df_intersection`)\n",
        "4. Add `is_negation` column to `df_intersection` with zero values.\n",
        "\n",
        "Mine negation examples\n",
        "\n",
        "5. Group the `df_merged` by left heads and relations, aggregate tails to set (let us call it `df_projection`)\n",
        "6. Merge `df_projection` and `df_merged` on head and relation (let us call it `df_negation_pre`)\n",
        "7. Group `df_negation_pre` by heads and relations from both previous datasets\n",
        "8. Aggregate tails from `df_projection` with `\"first\"` (let us call this column as `positive_tails`) and tails from `df_merged` with `set` (let us call this column as `negative_tails` and whole df as `df_negation`)\n",
        "9. For each row in `df_negation` remove `negative_tails` values from `positive_tails` (name it as `t`)\n",
        "10. Remove all lines from `df_negation` where the set of tails (`t`) is empty\n",
        "11. Add column `is_negation` with ones.\n",
        "\n",
        "\n",
        "12. Filter columns in `df_negation` and `df_intersection` to `[\"h_x\", \"r_x\", \"h_y\", \"r_y\", \"t\", \"is_negation\"]`\n",
        "13. Append `df_negation` to `df_intersection`\n",
        "14. Return numpy values of it\n",
        "\n",
        "Unencoded example (we change entity names to indices so it will look different)\n",
        "\n",
        "```\n",
        "[\n",
        "    [Head_1, Relation_1, Head_2, Relation_2, Tails, is_negation], # columns\n",
        "    [Russia, separated from, Yugoslavia, diplomatic relation, [Soviet Union], 0],\n",
        "    [Russia, diplomatic relation, Germany, diplomatic relation, [Germany, Republic of Abkhazia, South Ossetia, State of Palestine,], 1],\n",
        "    ...\n",
        "]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15iqIDt4sJWg",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-23caf4dc3e1a22f0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def mine_intersection_and_negation(df):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EedMi7eJKRc7"
      },
      "source": [
        "Now, we can mine queries and split our data into train, validation and test parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrcgygF9lHCF"
      },
      "outputs": [],
      "source": [
        "queries = mine_intersection_and_negation(df)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "perm = np.random.permutation(queries.shape[0])\n",
        "train_queries = queries[perm[: int(perm.shape[0] * 0.98)]]\n",
        "val_queries = queries[perm[int(perm.shape[0] * 0.98): int(perm.shape[0] * 0.99)]]\n",
        "test_queries = queries[perm[int(perm.shape[0] * 0.99):]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbd3O17bKRc9",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-46bad920e4041fac",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert queries.shape == (482642, 6)\n",
        "assert (\n",
        "    (queries[:, 0] == ent2id['German Empire']) &\n",
        "    (queries[:, 1] == rel2id['shares border with']) &\n",
        "    (queries[:, 2] == ent2id['County of Astarac']) &\n",
        "    (queries[:, 3] == rel2id['country']) &\n",
        "    (queries[:, 5] == 1)\n",
        ").sum() == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20OysOv7KRc-"
      },
      "source": [
        "## Task 2. Negation Dataset (2 points)\n",
        "\n",
        "Let us create the torch Dataset that will iterate over our query array and sample negatives.\n",
        "\n",
        "The first method, `sample_negative`, should sample a random entity_id that is not in the positive array (`tails` from the above example).\n",
        "\n",
        "The second one, `__getitem__`, should work as follows:\n",
        "\n",
        "1. Get row from query array\n",
        "2. Sample negative examples opposite to positive ones\n",
        "3. Select random positive example\n",
        "3. Convert `is_negation` flag with `1 - is_negation`\n",
        "4. Return the (head_1, relation_1, head_2, relation_2, converted_is_negation, positive sample, negative sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaK-us5UlHAC",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-29a3a71efc651c79",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "class NegationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, queries, n_ent):\n",
        "        self.queries = queries\n",
        "        self.n_ent = n_ent\n",
        "    \n",
        "    def sample_negative(self, positives):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfmb0gNKKRc_"
      },
      "source": [
        "Load queries to `NegationDataset` and torch DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faqgULUXlG9Z"
      },
      "outputs": [],
      "source": [
        "train_ds = NegationDataset(train_queries, len(ent2id))\n",
        "val_ds = NegationDataset(val_queries, len(ent2id))\n",
        "test_ds = NegationDataset(test_queries, len(ent2id))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=512, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT5ZPQ78KRc_",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-3f6c7097469c288d",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "anchor1, relation1, anchor2, relation2, neg_pow, positive, negative = train_ds.__getitem__(0)\n",
        "\n",
        "assert (anchor1, relation1, anchor2, relation2, neg_pow) == tuple(train_queries[0][:4].tolist() + [1 - train_queries[0][-1]])\n",
        "assert positive in train_queries[0, 4]\n",
        "assert negative not in train_queries[0, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy2nsnCns7t7"
      },
      "source": [
        "## Task 3. Model (6 points: 1.5 points per method and 1.5 for metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D2UDnTxKRdA"
      },
      "source": [
        "![](http://snap.stanford.edu/betae/model.png)\n",
        "\n",
        "The general idea of the BetaE method is to model queries with [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution). For example, let us consider the above image. The higher the Beta distribution density for a specific vector, the brighter the field on the space. We can think about it as a continuous box from the Query2Box model. In that work, we cover the hyperrectangular region to encode query where possible answer tails lie. Similarly, the distribution allows us to continuously highlight the most probable regions with answers.\n",
        "\n",
        "The beta distribution has two parameters: `alpha` and `beta` vectors. So, firstly, we will encode all our queries with two embedding vectors for each parameter. The `alpha` and `beta` parameters are always positive, so we will need to clamp vectors in some positive interval. It will be done with the `Regularizer` helper class.\n",
        "\n",
        "To handle intersection queries (`BetaIntersection` class), we will calculate center vectors similarly to the previous models from the seminar (GQE and Query2Box). We will take the attention-weighted mean of the input vectors.\n",
        "\n",
        "However, the projection procedure is different (`BetaProjection` class). It will be done through the MergeLayer, i.e. we concatenate the entity and relation vectors and pass them through the fully-connected network.\n",
        "\n",
        "We will copy helper classes from the original paper [implementation](https://github.com/snap-stanford/KGReasoning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr2EWnwms7dp"
      },
      "outputs": [],
      "source": [
        "class Regularizer:\n",
        "    def __init__(self, base_add, min_val, max_val):\n",
        "        self.base_add = base_add\n",
        "        self.min_val = min_val\n",
        "        self.max_val = max_val\n",
        "\n",
        "    def __call__(self, entity_embedding):\n",
        "        return torch.clamp(entity_embedding + self.base_add, self.min_val, self.max_val)\n",
        "\n",
        "\n",
        "class BetaIntersection(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(BetaIntersection, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.layer1 = nn.Linear(2 * self.dim, 2 * self.dim)\n",
        "        self.layer2 = nn.Linear(2 * self.dim, self.dim)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.layer1.weight)\n",
        "        nn.init.xavier_uniform_(self.layer2.weight)\n",
        "\n",
        "    def forward(self, alpha_embeddings, beta_embeddings):\n",
        "        all_embeddings = torch.cat([alpha_embeddings, beta_embeddings], dim=-1)\n",
        "        layer1_act = F.relu(self.layer1(all_embeddings))  # (num_conj, batch_size, 2 * dim)\n",
        "        attention = F.softmax(self.layer2(layer1_act), dim=0)  # (num_conj, batch_size, dim)\n",
        "\n",
        "        alpha_embedding = torch.sum(attention * alpha_embeddings, dim=0)\n",
        "        beta_embedding = torch.sum(attention * beta_embeddings, dim=0)\n",
        "\n",
        "        return alpha_embedding, beta_embedding\n",
        "\n",
        "class BetaProjection(nn.Module):\n",
        "    def __init__(self, entity_dim, relation_dim, hidden_dim, projection_regularizer, num_layers):\n",
        "        super(BetaProjection, self).__init__()\n",
        "        self.entity_dim = entity_dim\n",
        "        self.relation_dim = relation_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.layer1 = nn.Linear(self.entity_dim + self.relation_dim, self.hidden_dim)  # 1st layer\n",
        "        self.layer0 = nn.Linear(self.hidden_dim, self.entity_dim)  # final layer\n",
        "        for nl in range(2, num_layers + 1):\n",
        "            setattr(self, \"layer{}\".format(nl), nn.Linear(self.hidden_dim, self.hidden_dim))\n",
        "        for nl in range(num_layers + 1):\n",
        "            nn.init.xavier_uniform_(getattr(self, \"layer{}\".format(nl)).weight)\n",
        "        self.projection_regularizer = projection_regularizer\n",
        "\n",
        "    def forward(self, e_embedding, r_embedding):\n",
        "        x = torch.cat([e_embedding, r_embedding], dim=-1)\n",
        "        for nl in range(1, self.num_layers + 1):\n",
        "            x = F.relu(getattr(self, \"layer{}\".format(nl))(x))\n",
        "        x = self.layer0(x)\n",
        "        x = self.projection_regularizer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aArcFKxUKRdB"
      },
      "source": [
        "The final task is to define the simplified version of the BetaE model. The main difference from the original implementation is that it can handle only two types of queries described above.\n",
        "\n",
        "You will need to define three methods: `encode_projection`, `encode_query`, `calc_logit`.\n",
        "\n",
        "The `encode_projection` function projects the entity with given relation.\n",
        "\n",
        "It takes two arguments: a tensor with anchor ids (entities) and a tensor with relation ids. It works as follows:\n",
        "\n",
        "1. Select `self.entity_embedding` subset according to `anchor`\n",
        "2. Select `self.relation_embedding` subset according to `relation`\n",
        "3. Regularize entity embeddings from 1 with `self.entity_regularizer`\n",
        "4. Calculate projection embeddings with `self.projection_net`\n",
        "5. Split embedding from point 4 with `torch.chunk` on two parts (`alpha` and `beta` embeddings)\n",
        "\n",
        "The `calc_logit` function takes entity ids and beta distributions (`q`)\n",
        "\n",
        "1. Select `self.entity_embedding` subset according to `entities`\n",
        "2. Regularize entity embeddings from 1 with `self.entity_regularizer`\n",
        "3. Split embedding from point 2 with `torch.chunk` on two parts (alpha and beta embeddings)\n",
        "4. Calculate beta distributions (`torch.distributions.beta.Beta`) (let us call it `p`)\n",
        "5. Calculate logit using the following formula:\n",
        "\n",
        "$$\\text{logit} = \\gamma - \\Vert D_{KL}(p, q)\\Vert_1,$$\n",
        "\n",
        "where `p` and `q` are the Beta distributions. `p` is a distribution derived here in point 4, and `q` is the input parameter.\n",
        "\n",
        "The negation operator will be encoded in the `encode_query` method. The general idea is to take the reciprocal of the initial Beta distribution. We can do it by taking the `-1` power of the input `alpha` and `beta` embeddings.\n",
        "\n",
        "![](http://snap.stanford.edu/betae/betae.png)\n",
        "\n",
        "The `encode_query` method takes anchors and relations of two parts of query (anchor1, relation1, anchor2, relation2) and 1 - negation flag. It works as follows:\n",
        "\n",
        "1. Calculate projection alpha and beta embeddings for anchor1 and relation1\n",
        "2. Calculate projection alpha and beta embeddings for anchor2 and relation2\n",
        "3. Put alpha and beta embeddings from point 2 to the power of `neg_pow`\n",
        "4. Calculate whole query alpha and beta embeddings via `self.center_net` (stack alphas and betas, pass it to the function)\n",
        "5. Calculate beta distributions (`torch.distributions.beta.Beta`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3md9NOrDs7aq",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ddacf68c89e329ce",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class BetaE(nn.Module):\n",
        "    def __init__(self, gamma, entity_dim, nentity, nrelation, num_layers):\n",
        "        super(BetaE, self).__init__()\n",
        "        self.entity_dim = entity_dim\n",
        "        self.relation_dim = entity_dim\n",
        "        self.epsilon = 2.0\n",
        "        self.entity_regularizer = Regularizer(1, 0.05, 1e9)\n",
        "        self.projection_regularizer = Regularizer(1, 0.05, 1e9)\n",
        "        self.gamma = nn.Parameter(\n",
        "            torch.Tensor([gamma]),\n",
        "            requires_grad=False\n",
        "        )\n",
        "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim * 2))  # alpha and beta\n",
        "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
        "        self.__init_embeddings__()\n",
        "        self.center_net = BetaIntersection(self.entity_dim)\n",
        "        self.projection_net = BetaProjection(self.entity_dim * 2,\n",
        "                                             self.relation_dim,\n",
        "                                             self.entity_dim,\n",
        "                                             self.projection_regularizer,\n",
        "                                             num_layers)\n",
        "    \n",
        "    def __init_embeddings__(self):\n",
        "        emb_range = (self.gamma.item() + self.epsilon) / self.entity_dim\n",
        "        nn.init.uniform_(\n",
        "            self.entity_embedding,\n",
        "            a=-emb_range,\n",
        "            b=emb_range\n",
        "        )\n",
        "\n",
        "        nn.init.uniform_(\n",
        "            self.relation_embedding,\n",
        "            a=-emb_range,\n",
        "            b=emb_range\n",
        "        )\n",
        "    \n",
        "    def forward(self, anchor1, relation1, anchor2, relation2, neg_pow, positive, negative):\n",
        "        dists = self.encode_query(anchor1, relation1, anchor2, relation2, neg_pow)\n",
        "        return self.calc_logit(positive.reshape(-1, 1), dists), self.calc_logit(negative.reshape(-1, 1), dists)\n",
        "\n",
        "    def encode_projection(self, anchor, relation):\n",
        "        ### BEGIN SOLUTION\n",
        "        embedding = self.entity_regularizer(self.entity_embedding[anchor])\n",
        "        r_embedding = self.relation_embedding[relation]\n",
        "        embedding = self.projection_net(embedding, r_embedding)\n",
        "        return torch.chunk(embedding, 2, dim=-1)\n",
        "        ### END SOLUTION\n",
        "\n",
        "    def calc_logit(self, entities, dists):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def encode_query(self, anchor1, relation1, anchor2, relation2, neg_pow):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzg5G12SabJU"
      },
      "outputs": [],
      "source": [
        "model = BetaE(24, 800, len(ent2id), len(rel2id), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENT7ASYIKRdC",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-746e4b126e4ab10a",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "a, b = model.encode_projection(torch.LongTensor([0]), torch.LongTensor([0]))\n",
        "\n",
        "assert a.shape == torch.Size([1, 800])\n",
        "assert b.shape == torch.Size([1, 800])\n",
        "assert a.min().item() >= 0.05\n",
        "assert b.min().item() >= 0.05\n",
        "assert a.max().item() <= 1e9\n",
        "assert b.max().item() <= 1e9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0yrcbeFKRdC",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-e903e1a2fe7b6702",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "beta = torch.distributions.beta.Beta(torch.ones((1, 800)), torch.ones((1, 800)))\n",
        "l = round(model.calc_logit(torch.LongTensor([0]), beta).item(), 4)\n",
        "assert l > 23\n",
        "assert l < 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW6TzTmEKRdD",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-77ee9b2a447ce716",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "a = model.encode_query(torch.LongTensor([0]), torch.LongTensor([0]), torch.LongTensor([0]), torch.LongTensor([0]), torch.LongTensor([-1]))\n",
        "\n",
        "assert a.batch_shape == torch.Size([1, 1, 800])\n",
        "\n",
        "x = (a.concentration0 > a.concentration1).float().mean().item()\n",
        "assert (x > 0.45) and (x < 0.55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrPGIXa-CLKw"
      },
      "source": [
        "Let us define the training and evaluation loop. Our goal is to minimize the distance between the query embedding and positive answer embedding and maximize it between the query and negative answer. We can utilize the loss from Word2vec paper.\n",
        "\n",
        "$$L = - \\log{\\sigma (\\gamma - D_{KL}(positive, query))} - \\log{\\sigma (D_{KL}(negative, query) - \\gamma)}$$\n",
        "\n",
        "To evaluate our model, we will use the Accuracy@5. It means that we will predict the five closest answers to the query and check that a positive example lies within them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70xvu6wiBmZG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, opt, loader):\n",
        "    model.train()\n",
        "    loss_log = []\n",
        "    for idx, i in enumerate(tqdm(loader)):\n",
        "        anchor1, relation1, anchor2, relation2, neg_pow, positive, negative = [\n",
        "            j.to(device) for j in i\n",
        "        ]\n",
        "\n",
        "        pos, neg = model(anchor1, relation1, anchor2, relation2, neg_pow, positive, negative)\n",
        "        loss = - F.logsigmoid(pos).mean() - F.logsigmoid(-neg).mean()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        loss_log.append(loss.item())\n",
        "    return loss_log\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    entities = torch.repeat_interleave(torch.arange(len(ent2id)), 32).reshape(-1, 32).T\n",
        "    pos = 0\n",
        "    total = 0\n",
        "    for i in tqdm(loader):\n",
        "        anchor1, relation1, anchor2, relation2, neg_pow, positive, negative = [\n",
        "            j.to(device) for j in i\n",
        "        ]\n",
        "        dists = model.encode_query(anchor1, relation1, anchor2, relation2, neg_pow)\n",
        "        if anchor1.shape[0] < entities.shape[0]:\n",
        "            entities = entities[:anchor1.shape[0]]\n",
        "        logit = model.calc_logit(entities, dists)\n",
        "        topk = logit.argsort(descending=True)[:, :5]\n",
        "        pos += (positive.reshape(-1, 1) == topk).float().max(dim=1).values.sum().item()\n",
        "        total += positive.shape[0]\n",
        "    return pos / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNWA5i3WFwZK"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_tkwDZ6s7ZV"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "model = BetaE(24, 800, len(ent2id), len(rel2id), 2)\n",
        "model = model.to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), 3e-4)\n",
        "\n",
        "# train_loss_log = []\n",
        "# val_loss_log = []\n",
        "# acc_log = []\n",
        "# for epoch in range(25):\n",
        "#     loss = train(model, opt, train_loader)\n",
        "#     train_loss_log.append(np.mean(loss))\n",
        "#     acc = evaluate(model, val_loader)\n",
        "#     acc_log.append(acc)\n",
        "\n",
        "#     clear_output()\n",
        "#     plt.plot(train_loss_log, label=\"train\")\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "#     plt.plot(acc_log)\n",
        "#     plt.show()\n",
        "\n",
        "# with open(\"multihop-weights.pth\", \"wb\") as f:\n",
        "#     torch.save(model.state_dict(), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg2Z9LUuKRdE"
      },
      "source": [
        "Download your weights here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW8MaJtjKRdE",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-477b37f6110d1149",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "# model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um9c1L7uKRdF",
        "jupyter": {
          "outputs_hidden": true
        },
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-db5349ed1b0cc85c",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_ds = NegationDataset(test_queries[:200], len(ent2id))\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=True)\n",
        "score = evaluate(model, test_loader)\n",
        "print()\n",
        "print(f\"Test accuracy: {score:.4f}\")\n",
        "\n",
        "assert (score > 0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw3tyhicKRdF"
      },
      "source": [
        "Let us check some predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzNZSsgTKRdF"
      },
      "outputs": [],
      "source": [
        "id2ent = {j: i for i, j in ent2id.items()}\n",
        "\n",
        "dists = model.encode_query(\n",
        "    torch.LongTensor([ent2id[\"Turkmenistan\"]]).to(device),\n",
        "    torch.LongTensor([rel2id[\"shares border with\"]]).to(device),\n",
        "    torch.LongTensor([ent2id[\"Turkey\"]]).to(device),\n",
        "    torch.LongTensor([rel2id[\"shares border with\"]]).to(device),\n",
        "    torch.Tensor([-1]).to(device)\n",
        ")\n",
        "\n",
        "logit = model.calc_logit(torch.arange(len(ent2id)).reshape(1, -1), dists)\n",
        "[id2ent[i.item()] for i in logit.argsort(descending=True)[0, :5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVu5XAVBX-B7"
      },
      "outputs": [],
      "source": [
        "dists = model.encode_query(\n",
        "    torch.LongTensor([ent2id[\"Turkmenistan\"]]).to(device),\n",
        "    torch.LongTensor([rel2id[\"shares border with\"]]).to(device),\n",
        "    torch.LongTensor([ent2id[\"Turkey\"]]).to(device),\n",
        "    torch.LongTensor([rel2id[\"shares border with\"]]).to(device),\n",
        "    torch.Tensor([1]).to(device)\n",
        ")\n",
        "\n",
        "logit = model.calc_logit(torch.arange(len(ent2id)).reshape(1, -1), dists)\n",
        "[id2ent[i.item()] for i in logit.argsort(descending=True)[0, :5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn3dk3iGXwI2"
      },
      "outputs": [],
      "source": [
        "turkmenistan_neighbors = set(df[(df.h == ent2id[\"Turkmenistan\"]) & (df.r == rel2id[\"shares border with\"])].t)\n",
        "turkey_neighbors = set(df[(df.h == ent2id[\"Turkey\"]) & (df.r == rel2id[\"shares border with\"])].t)\n",
        "print(\"Neighbors intersection: \", ', '.join([id2ent[i] for i in turkmenistan_neighbors & turkey_neighbors]))\n",
        "print(\"Neighbors diff: \", ', '.join([id2ent[i] for i in turkmenistan_neighbors - turkey_neighbors]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH2RDmPPYHZE"
      },
      "source": [
        "The result looks like a pretty close prediction. Iran is within top-5 answers for intesection query. And it is not presented in top-5 negation query predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFveXiQWYaug"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N40nLUEtCLK0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}